import streamlit as st
import os
import openai

from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import ChatMessage

from pdfminer.high_level import extract_text
from dotenv import load_dotenv

# ğŸ” Load API Key
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# í™•ì¸ (ê°œë°œ ì¤‘ì—ë§Œ ì‚¬ìš©)
assert openai.api_key is not None, "âŒ OPENAI_API_KEYê°€ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¶ˆëŸ¬ì™€ì§€ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."

# ğŸ“„ PDF í…ìŠ¤íŠ¸ ì¶”ì¶œ
def get_pdf_text(filename):
    return extract_text(filename)

# ğŸ“š ë¬¸ì„œ ì²˜ë¦¬ ë° ì„ë² ë”©
def process_uploaded_file(FILE_PATH):
    if FILE_PATH is not None:
        raw_text = get_pdf_text(FILE_PATH)

        # í…ìŠ¤íŠ¸ ì²­í‚¹
        text_splitter = CharacterTextSplitter(
            separator="\n\n",
            chunk_size=10000,
            chunk_overlap=2000,
        )
        all_splits = text_splitter.create_documents([raw_text])
        print("ì´", len(all_splits), "ê°œì˜ ì²­í¬ ìƒì„±ë¨")

        # ë²¡í„°ìŠ¤í† ì–´ ìƒì„±
        vectorstore = FAISS.from_documents(all_splits, OpenAIEmbeddings())

        return vectorstore, raw_text
    return None, None

# ğŸ“¡ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥ í•¸ë“¤ëŸ¬
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

# ğŸ¤– ì§ˆë¬¸ì— ë‹µë³€ ìƒì„± (RAG)
def generate_response(query_text, vectorstore, callback):
    docs_list = vectorstore.similarity_search(query_text, k=3)
    docs = ""
    for i, doc in enumerate(docs_list):
        docs += f"'ë¬¸ì„œ{i+1}':{doc.page_content}\n"

    llm = ChatOpenAI(
        model_name="gpt-4o-mini",
        temperature=0,
        streaming=True,
        callbacks=[callback],
    )

    rag_prompt = [
        SystemMessage(
            content="ë„ˆëŠ” ë¬¸ì„œì— ëŒ€í•´ ì§ˆì˜ì‘ë‹µì„ í•˜ëŠ” 'ì•µë¬´ìƒˆ'ì•¼. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ì°¸ê³ í•´ì„œ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µë³€í•´ì¤˜. ë¬¸ì„œì— ì—†ìœ¼ë©´ ëª¨ë¥´ê² ë‹¤ê³  í•´ë„ ê´œì°®ì•„! ì´ëª¨í‹°ì½˜ë„ ì‚´ì§ ì¨ì¤˜. í•­ìƒ ë‹µë³€ ëì— ' ì§¹! ğŸ¦œ'ì´ë¼ê³  ë¶™ì—¬ì¤˜."
        ),
        HumanMessage(
            content=f"ì§ˆë¬¸:{query_text}\n\n{docs}"
        ),
    ]

    response = llm(rag_prompt)
    return response.content

# ğŸ“„ ìš”ì•½ ê¸°ëŠ¥
def generate_summarize(raw_text, callback):
    llm = ChatOpenAI(
        model_name="gpt-4o-mini",
        temperature=0,
        streaming=True,
        callbacks=[callback],
    )

    rag_prompt = [
        SystemMessage(content="ë‹¤ìŒ ë¬¸ì„œë¥¼ Notion ìŠ¤íƒ€ì¼ë¡œ ìš”ì•½í•´ì¤˜. í•µì‹¬ë§Œ, ì§§ê³  ëª…í™•í•˜ê²Œ."),
        HumanMessage(content=raw_text),
    ]

    response = llm(rag_prompt)
    return response.content

# ğŸ§¾ Streamlit ì¸í„°í˜ì´ìŠ¤
st.set_page_config(page_title='ğŸ¦œì–´ë–¤ ë¬¸ì„œë“  ë¬¼ì–´ë´!')
st.title('ğŸ¦œ ì–´ë–¤ ë¬¸ì„œë“  ë¬¼ì–´ë´!')

st.markdown("""
<style>
.main { background-color: #f5f5f5; }
.sidebar .sidebar-content { background-color: #f0f0f0; }
.stButton>button { background-color: #4CAF50; color: white; }
</style>
""", unsafe_allow_html=True)

# ğŸ“ íŒŒì¼ ì—…ë¡œë“œ
st.sidebar.header('ğŸ“„ ë¬¸ì„œ ì—…ë¡œë“œ')
uploaded_file = st.sidebar.file_uploader("PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”", type=["pdf"])

# ğŸ”„ ë²¡í„°ìŠ¤í† ì–´ ì²˜ë¦¬
if uploaded_file:
    vectorstore, raw_text = process_uploaded_file(uploaded_file)
    if vectorstore:
        st.session_state['vectorstore'] = vectorstore
        st.session_state['raw_text'] = raw_text

# ğŸ’¬ ì±„íŒ… ì´ˆê¸°í™”
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(
            role="assistant",
            content="ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ì—…ë¡œë“œí•œ ë¬¸ì„œë¥¼ ì´í•´í•˜ê³  ìš”ì•½í•˜ê±°ë‚˜ ì§ˆë¬¸ì— ë‹µë³€í•  ìˆ˜ ìˆëŠ” ì•µë¬´ìƒˆì˜ˆìš”. ğŸ¦œ ë¬´ì—‡ì´ ê¶ê¸ˆí•œê°€ìš”?"
        )
    ]

# ğŸ’¬ ì±„íŒ… ì¶œë ¥
for msg in st.session_state.messages:
    st.chat_message(msg.role).write(msg.content)

# ğŸ’¬ ì…ë ¥ ì²˜ë¦¬
if prompt := st.chat_input("ì˜ˆ: 'ìš”ì•½' ë˜ëŠ” 'ì´ ë¬¸ì„œì˜ ëª©ì ì€?'"):
    st.session_state.messages.append(ChatMessage(role="user", content=prompt))
    st.chat_message("user").write(prompt)

    with st.chat_message("assistant"):
        stream_handler = StreamHandler(st.empty())
        if prompt.strip() == "ìš”ì•½":
            response = generate_summarize(st.session_state['raw_text'], stream_handler)
        else:
            response = generate_response(prompt, st.session_state['vectorstore'], stream_handler)

    st.session_state.messages.append(ChatMessage(role="assistant", content=response))
