%%writefile app.py

import streamlit as st
import os

from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings
from langchain.chat_models import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from langchain.callbacks.base import BaseCallbackHandler
from langchain.schema import ChatMessage

# Function to extract text from an PDF file
from pdfminer.high_level import extract_text

from dotenv import load_dotenv
load_dotenv()

# loader
def get_pdf_text(filename):
    raw_text = extract_text(filename)
    return raw_text

# ë¬¸ì„œ ë¡œë“œ ë° ì²­í‚¹ (Loader + Splitter)
def process_uploaded_file(FILE_PATH):
    # Load document if file is uploaded
    if FILE_PATH is not None:

        # loader
        raw_text = get_pdf_text(FILE_PATH)

        # splitter
        text_splitter = CharacterTextSplitter(
            separator = "\n\n",     # TODO: ì–´ë–¤ ë¬¸êµ¬ë¥¼ ê¸°ì¤€ìœ¼ë¡œ? (ìŠ¤í˜ì´ìŠ¤ë°”, ì—”í„°, ',' ë“±)
            chunk_size = 1000,      # TODO: ë¬¸ì„œì˜ ì˜ë¦¼ í¬ê¸°ëŠ” ëª‡ìœ¼ë¡œ?
            chunk_overlap = 200,    # TODO: ê²¹ì¹˜ëŠ” ê¸¸ì´ëŠ” ëª‡ìœ¼ë¡œ?
        )
        all_splits = text_splitter.create_documents([raw_text])
        print("ì´ " + str(len(all_splits)) + "ê°œì˜ passage")

        # storage
        vectorstore = FAISS.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())

        return vectorstore, raw_text
    return None

# handle streaming conversation
class StreamHandler(BaseCallbackHandler):
    def __init__(self, container, initial_text=""):
        self.container = container
        self.text = initial_text

    def on_llm_new_token(self, token: str, **kwargs) -> None:
        self.text += token
        self.container.markdown(self.text)

# RAG ê¸°ë°˜ìœ¼ë¡œ ë‹µë³€ ìƒì„± (ê²€ìƒ‰(Retriever), ìƒì„±(Generator), ì—°ê²°(Chaining))
def generate_response(query_text, vectorstore, callback):

    # retriever
    docs_list = vectorstore.similarity_search(query_text, k=3) # TODO: ì—°ê´€ì„± ìˆëŠ” ë¬¸ì„œëŠ” ëª‡ ê°œ?!
    docs = ""
    for i, doc in enumerate(docs_list):
        docs += f"'ë¬¸ì„œ{i+1}':{doc.page_content}\n"

    # generator
    llm = ChatOpenAI(
        model_name="gpt-4o-mini",
        temperature=0,
        streaming=True,
        callbacks=[callback]
    )

    # chaining
    # prompt formatting
#     system_prompt = """
#     ë„ˆëŠ” ë¬¸ì„œì— ëŒ€í•´ ì§ˆì˜ì‘ë‹µì„ í•˜ëŠ” ì¡°êµì•¼. \n
# ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€ì„ í•´ì¤˜. \n
# ë¬¸ì„œì— ë‚´ìš©ì´ ì •í™•í•˜ê²Œ ë‚˜ì™€ìˆì§€ ì•Šìœ¼ë©´ ëŒ€ë‹µí•˜ì§€ ë§ˆ.

# ë„ˆëŠ” ê´´íŒí•œ ì„±ê²©ì„ ê°€ì§€ê³  ìˆê³  ì™„ì „ ì¸¤ë°ë˜ì•¼."""

    rag_prompt = [
        SystemMessage(
            # content=system_prompt,
            content="ë„ˆëŠ” ë¬¸ì„œì— ëŒ€í•´ ì§ˆì˜ì‘ë‹µì„ í•˜ëŠ” 'ì¡°êµ'ì•¼. ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€ì„ í•´ì¤˜. ë¬¸ì„œì— ë‚´ìš©ì´ ì •í™•í•˜ê²Œ ë‚˜ì™€ìˆì§€ ì•Šìœ¼ë©´ ëŒ€ë‹µí•˜ì§€ ë§ˆ. ì´ëª¨í‹°ì½˜ì„ ì‚¬ìš©í•´ì„œ ì¹œê·¼í•˜ê²Œ ë‹µë³€í•´ì¤˜!"
        ),
        HumanMessage(
            content=f"ì§ˆë¬¸:{query_text}\n\n{docs}"
        ),
    ]

    response = llm(rag_prompt)
    return response.content

def generate_summarize(raw_text, callback):

    # generator
    llm = ChatOpenAI(model_name="gpt-4o-mini", temperature=0, streaming=True, callbacks=[callback])

    # prompt formatting
    rag_prompt = [
        SystemMessage(
            content="ë‹¤ìŒ ë‚˜ì˜¬ ë¬¸ì„œë¥¼ 'Notion style'ë¡œ ìš”ì•½í•´ì¤˜. ì¤‘ìš”í•œ ë‚´ìš©ë§Œ."
        ),
        HumanMessage(
            content=raw_text
        ),
    ]

    response = llm(rag_prompt)
    return response.content

# page title
st.set_page_config(page_title='ğŸ¦œì–´ë–¤ ë¬¸ì„œë“  ë¬¼ì–´ë´!ğŸ¦œ')
st.title('ğŸ¦œì–´ë–¤ ë¬¸ì„œë“  ë¬¼ì–´ë´!ğŸ¦œ')

# file upload --> PDFë§Œ ë°›ì„ ìˆ˜ ìˆë„ë¡ í•œë‹¤.
# uploaded_file = st.file_uploader('Upload an document', type=['hwp','pdf'])
st.markdown("""
	<style>
	.main { background-color: #f5f5f5; }
	.sidebar .sidebar-content { background-color: #f0f0f0; }
	.stButton>button { background-color: #4CAF50; color: white; }
	</style>
	""", unsafe_allow_html=True)

# íŒŒì¼ ì—…ë¡œë“œ
st.sidebar.header('ğŸ“„ íŒŒì¼ ì—…ë¡œë“œ')
uploaded_file = st.sidebar.file_uploader('ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš”!', type=['pdf'])

# file upload logic
if uploaded_file:
    vectorstore, raw_text = process_uploaded_file(uploaded_file)

    # ì´ˆê¸°ì— í•œ ë²ˆ ë§Œë“¤ê³  session_stateì— ë³´ê´€ --> session_stateì— ì ‘ê·¼í•´ì„œ ë°˜ë³µí•˜ì—¬ í™œìš©
    if vectorstore:
        st.session_state['vectorstore'] = vectorstore
        st.session_state['raw_text'] = raw_text

# chatbot greatings
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(
            role="assistant", content="ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ë¬¸ì„œì— ëŒ€í•œ ì´í•´ë¥¼ ë„ì™€ì£¼ëŠ” ì•µë¬´ìƒˆì…ë‹ˆë‹¤! ì–´ë–¤ê²Œ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?!"
        )
    ]

# conversation history print --> ì£¼ì„ ì²˜ë¦¬í•˜ê³  ë¹„êµí•´ë³´ê¸°
for msg in st.session_state.messages:
    st.chat_message(msg.role).write(msg.content)

# message interaction
if prompt := st.chat_input("'ë¬¸ì„œ ìš”ì•½ í•´ì¤˜'ë¼ê³  ì…ë ¥í•´ë³´ì„¸ìš”!"):
    st.session_state.messages.append(ChatMessage(role="user", content=prompt))
    st.chat_message("user").write(prompt)

    with st.chat_message("assistant"):
        stream_handler = StreamHandler(st.empty())

        if prompt == "ìš”ì•½":
            response = generate_summarize(st.session_state['raw_text'],stream_handler)

        else:
            response = generate_response(prompt, st.session_state['vectorstore'], stream_handler)

    st.session_state.messages.append(
        ChatMessage(role="assistant", content=response)
    )
